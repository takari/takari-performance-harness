<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Performance Tests HowTo</title>
  <link rel="stylesheet" href="http://dev.eclipse.org/default_style.css"
 type="text/css">
</head>
<body>
<h1>Performance Tests HowTo</h1>
2004/10/15<br>
<br>
The Eclipse performance test plugin (org.eclipse.test.performance)
provides infrastructure for instrumenting programs
to collect performance data and to assert that performance doesn't drop
below a baseline. The infrastructure is supported on Windows, Linux,
and MacOS X.<br>
<br>
The first part of this document describes how performance tests are
written and executed, the <a href="#cloudscape">second part</a> explains how performance data is
collected in a database and how this database is installed and
configured.<br>
<h2><br>
</h2>
<h2>Writing Performance Tests</h2>
<h3>Setting up the environment</h3>
<ul>
  <li>check out the following plug-ins from dev.eclipse.org.</li>
  <ul>
    <li>org.eclipse.test.performance</li>
    <li>org.eclipse.test.performance.win32 (for Windows only)</li>
  </ul>
  <li>add org.eclipse.test.performance to your test plug-in's
dependencies</li>
</ul>
<h3>Writing a performance test case</h3>
<ul>
  <li>also see below for <a href="#convert">converting test cases from
earlier versions of this document</a></li>
  <li>create a test case with test methods along the lines of:</li>
<pre>public void testMyOperation() {
	Performance perf= Performance.getDefault();
	PerformanceMeter performanceMeter= perf.<b>createPerformanceMeter</b>(perf.getDefaultScenarioId(this));
	try {
		for (int i= 0; i &lt; 10; i++) {
			performanceMeter.<b>start</b>();
			toMeasure();
			performanceMeter.<b>stop</b>();
 		}
		performanceMeter.<b>commit</b>();
		perf.<b>assertPerformance</b>(performanceMeter);
 	} finally {
		performanceMeter.<b>dispose</b>();
 	}
 }
</pre>
  <li>or create a test case extending <code>PerformanceTestCase</code>
which is a convenience class that makes the
use of <code>PerformanceMeter</code> transparent:
<pre>public class MyPerformanceTestCase extends PeformanceTestCase {

	public void testMyOperation() {
		for (int i= 0; i &lt; 10; i++) {
			<b>startMeasuring</b>();
			toMeasure();
			<b>stopMeasuring</b>();
		}
		<b>commitMeasurements</b>();
		<b>assertPerformance</b>();
	}
}
</pre>
  </li>
</ul>
Notes:
<ul>
  <li><code>Performance</code> is currently just a factory for <code>PerformanceMeter</code>s.
The scenario id passed to <code>createPerformanceMeter(...)</code> has
to be unique in a single test run and should be the same in each test
run.</li>
  <li>The <code>Peformance#getDefaultScenarioId(...)</code> methods
are provided for convenice. </li>
  <li><code>PerformanceMeter</code> is a general abstraction for
measuring, that supports repeated measurements by multiple invocations
of the <code>start()</code>, <code>stop()</code> sequence. The call
to <code>commit()</code> is required before evaluation with <code>assertPerformance()</code>
and <code>dispose()</code> is required before releasing the meter. </li>
  <li>The number of runs will need some tuning, such that the last runs
are stable. In general the first runs will take more time because the
code is not optimized by the JIT compiler yet. This deserves extra
caution when other code is run before the measurements (e.g., if tests
are added, removed, reordered). </li>
</ul>
<h3>Participating in the performance summary (aka "Performance
Fingerprint")</h3>
If the number of performance tests grows large, it becomes harder to
get a good overview of the performance characteristics of a build. A
solution for this problem is a&nbsp; performance summary graph that
tries to condense a small subset of key performance tests into a graph
that fits onto a single page. Every Eclipse component can mark one or
two tests for inclusion in this performance summary. The bar graph
shows the performance development of about 20 tests relative to a
reference build in an easy to grasp red/green presentation.<br>
<br>
<img style="width: 800px; height: 282px;" alt="summary graph"
 src="FP_I200409240800_I200410050800.jpeg"><br>
<br>
Marking a test for inclusion is done by passing a performance meter
into the method Performance.tagAsGlobalSummary(...). This method should
be called outside of start/stop calls but it must be called before the
the call to commit().<br>
<pre>// ....<br>Performance perf= Performance.getDefault();<br>PerformanceMeter pm= perf.createPerformanceMeter(perf.getDefaultScenarioId(this));<br>perf.<span
 style="font-weight: bold;">tagAsGlobalSummary</span>(pm, "A Short Name", Dimension.CPU_TIME);<br>try {<br>	// ...<br></pre>
In order to keep the overview graph small, only a single dimension of
the test's data is shown and only a short name is used to label the
data (instead of the rather long scenario ID). Both the short label as
well as one (or more) dimensions must be supplied in the call to
tagAsGlobalSummary.<br>
<br>
The PerformanceTestCase provides a similar method that must be called
before startMeasuring(): <br>
<pre>public class MyPerformanceTestCase extends PeformanceTestCase {

	public void testMyOperation() {
		<b>tagAsGlobalSummary</b>("A Short Name", Dimension.CPU_TIME);
		for (int i= 0; i &lt; 10; i++) {
			startMeasuring();
			toMeasure();
			stopMeasuring();
		}
		commitMeasurements();
		assertPerformance();
	}
}</pre>
<h3>Running a performance test case (from a launch configuration)</h3>
<ul>
  <li>create a new JUnit Plug-in Test launch configuration for the test
case</li>
  <li>add "-Xms250M -Xmx250M" or similar to the VM arguments to avoid
memory pressure during the measurements</li>
  <li>run the launch configuration</li>
  <li>by default&nbsp;the measured averages of the performance monitor
are written to the console on <code>commit()</code>. This is
surpressed if performance tests are configured to store data in the
database (see below).<br>
  </li>
</ul>
<h3>Running a performance test case (within the Automated Testing
Framework on each build)</h3>
<ul>
  <li>if the <code>test.xml</code> of your test plug-in already exists
and looks similar to the <code>jdt.text.tests</code>' one, add targets
similar to:</li>
  <pre>&lt;!-- This target defines the performance tests that need to be run. --&gt;<br>&lt;target name="performance-suite"&gt;<br>  &lt;property name="your-performance-folder" value="${eclipse-home}/your_performance_folder"/&gt;<br>  &lt;delete dir="${your-performance-folder}" quiet="true"/&gt;<br>  &lt;ant target="ui-test" antfile="${library-file}" dir="${eclipse-home}"&gt;<br>    &lt;property name="data-dir" value="${your-performance-folder}"/&gt;<br>    &lt;property name="plugin-name" value="${plugin-name}"/&gt;<br>    &lt;property name="classname" value="<em>&lt;your fully qualified test case class name&gt;</em>"/&gt;<br>  &lt;/ant&gt;<br>&lt;/target&gt;<br><br>&lt;!-- This target runs the performance test suite. Any actions that need to happen --&gt;<br>&lt;!-- after all the tests have been run should go here. --&gt;<br>&lt;target name="performance" depends="init,performance-suite,cleanup"&gt;<br>  &lt;ant target="collect" antfile="${library-file}" dir="${eclipse-home}"&gt;<br>    &lt;property name="includes" value="org*.xml"/&gt;<br>    &lt;property name="output-file" value="${plugin-name}.xml"/&gt;<br>  &lt;/ant&gt;<br>&lt;/target&gt;<br></pre>
</ul>
Notes:
<ul>
  <li>Performance measurements are run on a dedicated performance
measurement machine of the Releng team.</li>
  <li>If you have created a new source folder, do not forget to include
it in the build (add it to the "build.properties" file).</li>
</ul>
<h3>Running a performance test case (within the Automated Testing
Framework, locally)</h3>
<ul>
  <li>modify <code>test.xml</code> as described above </li>
  <li>download a build and its accompanying Automated Testing Framework</li>
  <li>unzip the Automated Testing Framework (creates a directory
"eclipse-testing", see also the contained readme.html)
    <ul>
      <li>you need <a
 href="http://www.info-zip.org/pub/infozip/UnZip.html">Info-ZIP UnZip</a>
version 5.41 or later (<a
 href="ftp://sunsite.cnlab-switch.ch/mirror/infozip/WIN32/unz551xN.exe">WinNT</a>)
installed and added to the path</li>
    </ul>
  </li>
  <li>put the build's zip in the eclipse-testing directory</li>
  <li>on the command prompt of Windows (similar for other platforms,
see the readme.html of the Automated Testing Framework):</li>
  <ul>
    <li>change to the eclipse-testing directory</li>
    <li>run: <code>runtests.bat -vm <em>&lt;path to JRE&gt;</em>\bin\java.exe
      <em>&lt;your test target&gt;</em><code></code></code></li>
    <li>if your test plug-in is already run as part of the Automatic
Test Framework, your test target can be found in the eclipse-testing
directory's readme.html or test.xml</li>
    <li>you can stop the test run as soon as the framework has
extracted everything and started running test cases</li>
  </ul>
  <li>export your test plug-in as 'Deployable plug-in' to the
eclipse-testing/test-eclipse/eclipse directory; choose to export as
directory structure</li>
  <li>(if you have exported a 3.1.0 plug-in to a 3.0.0 test harness,
delete the old 3.0.0 test plug-in and rename yours to *_3.0.0)</li>
  <li>on the command prompt:</li>
  <ul>
    <li>change to the eclipse-testing directory</li>
    <li>run <em>(note the additional -noclean argument)</em>: <code>runtests.bat
      <span style="color: red;">-noclean</span> -vm <em>&lt;path to
JRE&gt;</em>\bin\java.exe <em>&lt;your test target&gt;</em><code></code></code></li>
  </ul>
</ul>
<h3><a name="convert">Converting test cases from earlier versions of
this document</a></h3>
You can skip the following list if you just start writing performance
tests and don't have to convert "legacy" code.<br>
<ul>
  <li>replace the use of any <code>PerformanceMeterFactory</code> with
    <code>org.eclipse.test.performance.Performance#createPerformanceMeter(...)</code>,
maybe in combination with one of the <code>Performance#getDefaultScenarioId(...)</code>
methods </li>
  <li>replace the use of any old <code>PerformanceMeter</code> with
org.eclipse.test.performance.PerformanceMeter:</li>
  <ul>
    <li>add a call to <code>Performance.assertPerformance(performanceMeter)</code>
after the call to <code>commit()</code></li>
    <li>surround the use of the <code>PerformanceMeter</code> with a
try-block and add a call to <code>performanceMeter.dispose()</code> in
the finally-block</li>
  </ul>
  <li>remove <code>org.eclipse.perfmsr.core</code> and <code>org.eclipse.jdt.text.tests</code>
from your test plug-in's dependencies</li>
  <li>for running a performance test from a launch configuration:</li>
  <ul>
    <li>remove the <code>etools_perf_ctrl</code> system property from
your launch configuration's VM arguments</li>
    <li>instead add the <code>-DInternalPrintPerformanceResults</code>
to your launch configuration's VM arguments, this option is a
provisional solution for providing access to the measurements, it lets
the <code>PerformanceMeter</code> print the measured averages to the
console on <code>commit()</code> and will be removed as soon as the
new backend is in place</li>
  </ul>
  <li>for running a performance test within the Automated Testing
Framework on each build (if your <code>test.xml</code> looks similar
to the <code>jdt.text.tests</code>' one):</li>
  <ul>
    <li>remove any <code>if="performance"</code> and <code>unless="performance"</code>
attributes</li>
    <li>remove the dependency of the <code>run</code> target on the <code>performance-suite</code>
target</li>
    <li>duplicate the <code>run</code> target, rename the copy to <code>performance</code>
and make it dependent on <code>performance-suite</code> instead of <code>suite</code></li>
  </ul>
  <li>for running a performance test locally within the Automated
Testing Framework (instructions for Windows):</li>
  <ul>
    <li>when running <code>runtests.bat</code>, remove the <code>eclipse-testing.properties</code>
file and omit the <code>-properties ../eclipse-testing.properties</code>
option</li>
  </ul>
</ul>
<h2><br>
</h2>
<h2><a name="cloudscape">Setting up the Cloudscape database</h2>
<p>Performance tests are only valuable if measured data can be
monitored over time and compared against reference data. For this
functionality the Eclipse performance plugin makes use of IBM's <a
 href="http://www-306.ibm.com/software/data/cloudscape/">Cloudscape</a>
database (which is now becoming the Apache subproject named <a
 href="http://incubator.apache.org/derby/">Derby</a>).<br>
</p>
<p>Cloudscape is a database engine written in Java that can be accessed
via JDBC. Cloudscape is easily embeddable in Java programs or can run
as a network server. <br>
</p>
<p>This section describes how to install Cloudscape and how to
configure
the performance test plugin to use Cloudscape.
</p>
<p> </p>
<h3>Getting and installing Cloudscape</h3>
<p>The performance plugin has an optional prereq for a "Cloudspace"
library project. Since it is optional, you won't see any compile time
errors when loading the performance plugin from the Eclipse repository
and the Cloudscape project is not available in your workspace. However
you'll see runtime errors when running the tests and trying to access
the database.<br>
</p>
If you have access to the following repository you can get the
"Cloudscape" library project from there:<br>
<pre>  :pserver:anonymous@ottcvs1.ott.oti.com:/home/cvs/zrheclipse<br></pre>
Otherwise get Cloudscape from
<a
 href="http://www-106.ibm.com/developerworks/db2/library/techarticle/dm-0408cline/index.html">here</a>.
Unpack the archive to any directory.<br>
To create a library project for Cloudscape, open the Java project
wizard and enter "Cloudscape" as the project's name. Go to the next
page and select the "Libraries" tab. Remove the JRE and add the five
jar-files from Cloudscape's lib directory via the "Add External JARs"
button. Switch to the "Order and Export" tab and check all five
libraries. Press "Finish". Create a new file "plugin.xml" inside the
Cloudscape project and paste the following contents into it:<br>
<pre>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?eclipse version="3.0"?&gt;
&lt;plugin
&nbsp;&nbsp; id="Cloudscape"
&nbsp;&nbsp; name="Cloudscape"
&nbsp;&nbsp; version="1.0.0"&gt;
&nbsp;&nbsp; &lt;runtime&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;library name="db2jcc.jar"&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;export name="*"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/library&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;library name="db2jcc_license_c.jar"&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;export name="*"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/library&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;library name="cstools.jar"&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;export name="*"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/library&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;library name="csnet.jar"&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;export name="*"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/library&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;library name="cs.jar"&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;export name="*"/&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/library&gt;
&nbsp;&nbsp; &lt;/runtime&gt;
&lt;/plugin&gt;
</pre>
<p>In addition you'll need to load the performance plugin
(org.eclipse.test.performance) and if you are running
on Windows the associated fragment
(org.eclipse.test.performance.win32).<br>
</p>
<h3>Configuring the performance plugin for using Cloudscape<br>
</h3>
The performance test plugin is configured via the three Java properties
eclipse.perf.dbloc, eclipse.perf.config, and eclipse.perf.assertAgainst.<br>
<br>
The eclipse.perf.dbloc specifies where the Cloudscape DB is located.
If no value is given
<pre>
	-Declipse.perf.dbloc=
</pre>
Cloudscape runs in embedded mode (not as a separate server)
and the DB will live in your home directory.<br>
If an absolute or relative path is given, Cloudscape uses or creates
the DB in that location. E.g. with
<pre>
	-Declipse.perf.dbloc=/tmp/cloudscape
</pre>
Cloudscape runs in embedded mode and creates the database under
/tmp/cloudscape.<br>
To connect to a cloudscape server running locally or remotely use the
following:
<pre>
	-Declipse.perf.dbloc=net://<i>tcp-ip address</i>
</pre>
With the properties eclipse.perf.config and eclipse.perf.assertAgainst
you specify the name under which performance data is stored in the
database and the name of the reference data to compare against. This
"name" is not a single string but a set of key/value pairs separated by
semicolons:<br>
<pre>
	-Declipse.perf.config=<i>key1</i>=<i>value1</i>;<i>key2</i>=<i>value2</i>;...;<i>keyn</i>=<i>valuen</i>
	-Declipse.perf.assertAgainst=<i>key1</i>=<i>value1</i>;<i>key2</i>=<i>value2</i>;...;<i>keyn</i>=<i>valuen</i>
</pre>
<p>The key/value pairs can be used to associate the collected
performance data with information about the configuration that was used
to generate the data. Typically this includes the name of the build,
the system on which the test were run, or the used Java VM. So in this
example: </p>
<pre>
	-Declipse.perf.config=build=N20040914;host=relengwin;jvm=j9
</pre>
performance data for the nightly build N20040914 is stored in the
database under a "name" that consist of three key/value pairs.<br>
If the tests are run multiple times with the same arguments, the new
data does not replace
old data but is added under the same name. Programs that visualize the
data are expected
to aggregate the data for example by calculating the average of all
tests.
<br>
<br>
To assert that performance data collected for another build does not
degrade with respect to some reference data the assertAgainst property
is used similarly:
<pre>
	-Declipse.perf.assertAgainst=build=R3.0;host=relengwin;jvm=j9
</pre>
This property enables any "assertPerformance" calls in your performance
tests and compares the newly measured data against the data specified
by the three key/value pairs. Please note that the order of the pairs
does not matter when looking up the data in the database. However, the
number of key/value pairs must be identical.<br>
<br>
Because in most cases you want to store newly collected data as well as
assert against other reference data at the same time you'll need to
specify both properties. In this case only those key/value pairs must
be listed in the assertAgainst property, that differ from the config
property:
<pre>
	-Declipse.perf.config=build=N20040914;host=relengwin
	-Declipse.perf.assertAgainst=build=R3.0
</pre>
So in the example from above the new performance data is stored in the
database under the build name
"N20040914" and the host "relengwin" and the "assertPerformance"
compares this data against data tagged with a build name of "R3.0" and
an implicitely specified host "relengwin".<br>
<br>
If you want to assert the average of multiple runs (instead of the data
of a single run)
against the reference data, do the following:<br>
<pre>
	// Run program 4 times to collect data under build name "I20040914"
	... -Declipse.perf.config=build=I20040914
	... -Declipse.perf.config=build=I20040914
	... -Declipse.perf.config=build=I20040914
	... -Declipse.perf.config=build=I20040914
	
	// Run program a 5th time and collect more data under I20040914
	// and assert the average of 5 runs of I20040914 against some baseline data
	... -Declipse.perf.config=build=I20040914 -Declipse.perf.assertAgainst=build=R3.0
</pre>
<h3>Viewing the data</h3>
Since we do not (yet) have fancy visualization tools, the performance
test plugin provides two classes
that can be run as standalone programs for "dumping" the data contained
in the database.
For both programs you need to specify the database location via the <code>eclipse.perf.dbloc</code>
property
(most easily done via a launch configuration).
<ul>
  <li>To view the data in a tabular format use the class <code>org.eclipse.test.internal.performance.db.View</code>.
It produces the following output:
<pre>
Scenario: org.eclipse.jdt.ui.tests.performance.views.PackageExplorerPerfTest#testOpen()
Builds:                 base          b0003       b0004       b0007       b0008          b0009
Kernel time:       30 ms [0]     10 ms [10]    0 ms [0]    0 ms [0]   20 ms [0]   13 ms [12.5]
Elapsed Process:  325 ms [0]  491 ms [94.5]  358 ms [0]  296 ms [0]  316 ms [0]  304 ms [19.6]
User time:        240 ms [0]    305 ms [45]  270 ms [0]  270 ms [0]  260 ms [0]   240 ms [8.2]
System Time:      325 ms [0]  491 ms [94.5]  358 ms [0]  296 ms [0]  316 ms [0]  304 ms [19.6]
CPU Time:         270 ms [0]    315 ms [55]  270 ms [0]  270 ms [0]  280 ms [0]   253 ms [4.7]
</pre>
If you don't want to see all data in the database you can modify the
three SQL wildcards ("%") in the call to <code>DB.queryScenarios</code>.
    <p> </p>
  </li>
  <li>To dump the (quite voluminous) contents of all database tables
run <code>org.eclipse.test.internal.performance.db.DB</code>. </li>
</ul>
<h3>How to setup a Cloudscape server (on Linux and MacOS X)</h3>
<ul>
  <li>Either get Cloudscape from the repository or from <a
 href="http://www-106.ibm.com/developerworks/db2/library/techarticle/dm-0408cline/index.html">here</a>.
    <p> </p>
  </li>
  <li>Get the (Bourne) shell script "cs.sh" from the scripts folder of <code>org.eclipse.test.performance</code>
and install it on the server
(rename it to "cs" and make it executable; if you've checked out the
file on Windows and copied it to Linux,
it might be necessary to convert line delimiters with the dos2unix
tool).
The script simplifies the usage of the Cloudscape tools - especially
starting and stopping the server - because
it sets the correct classpath and some important properties. </li>
  <li>Edit the script and adapt the variables <code>CSLIB</code>, <code>DBROOT</code>,
and <code>JAVA</code>
to your installation. <code>CSLIB</code> should point to the directory
containing the Cloudscape jars.
If you've used the Cloudscape installer, then this is the lib directory
inside the Cloudscape 10.0 directory.
If you are using the Cloudscape project from the repository, then this
is just the project folder. </li>
  <li>in a Shell execute
<pre>
	cs start &amp;
</pre>
to launch the server in background. The server will send this to
stdout:
<pre>
	Server is ready to accept connections on port 1527.
</pre>
to the console. </li>
  <p> </p>
  <li>stop the server with
<pre>
	cs stop
</pre>
  </li>
</ul>
</body>
</html>
